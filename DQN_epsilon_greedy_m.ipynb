{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory_size =100_000\n",
    "        self.discount = 0.95    # 折扣率\n",
    "        self.epsilon = 1.0  # 探索率\n",
    "        self.learning_rate = 0.001\n",
    "        self.update_interval = 4\n",
    "        self.optimizer = Adam(learning_rate = self.learning_rate)\n",
    "        self.q_network= self._build_network()\n",
    "        self.target_q_network = self._build_network()\n",
    "        \n",
    "    \n",
    "    def _build_network(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.state_size,)))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        return model\n",
    "\n",
    "    def evaluate_loss(self, experiences):\n",
    "        states, actions, rewards, next_states, done_flags = experiences\n",
    "        \n",
    "        q_next = self.target_q_network(next_states)\n",
    "        max_q_next = tf.math.reduce_max(q_next, axis=1)\n",
    "        expected_q_values = rewards + self.discount * (1 - done_flags) * max_q_next\n",
    "\n",
    "        q_values = self.q_network(states)\n",
    "        action_indices = tf.range(0, tf.shape(q_values)[0]) * tf.shape(q_values)[1] + tf.cast(actions, tf.int32)\n",
    "        relevant_q_values = tf.gather(tf.reshape(q_values, [-1]), action_indices)\n",
    "\n",
    "        loss_value = tf.reduce_mean(tf.square(expected_q_values - relevant_q_values))\n",
    "        return loss_value\n",
    "\n",
    "    def update_target_network(self):\n",
    "        update_factor = 0.001\n",
    "        target_params = self.target_q_network.weights\n",
    "        main_params = self.q_network.weights\n",
    "\n",
    "        for target, main in zip(target_params, main_params):\n",
    "            updated_weights = update_factor * main + (1 - update_factor) * target\n",
    "            target.assign(updated_weights)\n",
    "            \n",
    "    @tf.function\n",
    "    def agent_learn(self, experiences):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.evaluate_loss(experiences)\n",
    "        parameters = self.q_network.trainable_variables\n",
    "        gradients = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, parameters))\n",
    "        self.update_target_network()\n",
    "\n",
    "    def select_action(self,state):\n",
    "        if random.random()<=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.q_network(state).numpy()[0])\n",
    "            \n",
    "    \n",
    "    def fetch_sampled_experiences(self, experience_buffer):\n",
    "        sampled_experiences = random.sample(experience_buffer, k=64)\n",
    "        state_list = [exp.state for exp in sampled_experiences if exp is not None]\n",
    "        action_list = [exp.action for exp in sampled_experiences if exp is not None]\n",
    "        reward_list = [exp.reward for exp in sampled_experiences if exp is not None]\n",
    "        next_state_list = [exp.next_state for exp in sampled_experiences if exp is not None]\n",
    "        done_list = [exp.terminated for exp in sampled_experiences if exp is not None]\n",
    "\n",
    "        states_tensor = tf.convert_to_tensor(state_list, dtype=tf.float32)\n",
    "        actions_tensor = tf.convert_to_tensor(action_list, dtype=tf.float32)\n",
    "        rewards_tensor = tf.convert_to_tensor(reward_list, dtype=tf.float32)\n",
    "        next_states_tensor = tf.convert_to_tensor(next_state_list, dtype=tf.float32)\n",
    "        done_tensor = tf.convert_to_tensor(np.array(done_list).astype(np.uint8), dtype=tf.float32)\n",
    "\n",
    "        return states_tensor, actions_tensor, rewards_tensor, next_states_tensor, done_tensor\n",
    "\n",
    "    def get_one_hot_encoding(self, state):\n",
    "        state_vector = np.zeros(self.state_size)\n",
    "        state_vector[state] = 1\n",
    "        \n",
    "        return state_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Agent(Agent, episodes, rewards, state_size):\n",
    "    memory_buffer = deque(maxlen=Agent.memory_size)\n",
    "    Agent.target_q_network.set_weights(Agent.q_network.get_weights())\n",
    "    min_epsilon=0.01\n",
    "    e_decay=0.05\n",
    "    points_history = []\n",
    "    time_step = 1000\n",
    "    experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"terminated\"])\n",
    "     \n",
    "    for i in range(episodes):\n",
    "        observation = env.reset()\n",
    "        state = observation[0]\n",
    "        state = Agent.get_one_hot_encoding(state)\n",
    "        total_rewards = 0\n",
    "        timesup = False\n",
    "        step = 0\n",
    "          \n",
    "        while not timesup:\n",
    "            state_qn = np.reshape(state, [1, state_size])\n",
    "            action=Agent.select_action(state_qn)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            next_state = Agent.get_one_hot_encoding(next_state)\n",
    "            memory_buffer.append(experience(state, action, reward, next_state, terminated))\n",
    "            if (step+1) % Agent.update_interval == 0 and len(memory_buffer) > 64:\n",
    "                experiences = Agent.fetch_sampled_experiences(memory_buffer)\n",
    "                Agent.agent_learn(experiences)\n",
    "            \n",
    "            state = next_state\n",
    "            total_rewards += reward\n",
    "            step += 1\n",
    "            \n",
    "            if step == time_step:\n",
    "                timesup = True\n",
    "\n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "        points_history.append(total_rewards)\n",
    "        avg_points = np.mean(points_history[-100:])\n",
    "\n",
    "        #Update epsilon\n",
    "        Agent.epsilon = max(min_epsilon,e_decay*Agent.epsilon)\n",
    "\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\", end=\"\")\n",
    "\n",
    "\n",
    "        if(avg_points >= 8):\n",
    "            print(f\"Environment solved in {i+1} episodes!\")\n",
    "            break\n",
    "            \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\")\n",
    "        rewards.append(total_rewards)\n",
    "        \n",
    "    env.close()\n",
    "    print(f\"\\rTraining completed over {episodes} episodes\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 | Total point average of the last 100 episodes: -1226.40"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "rewards = []\n",
    "episodes=2000\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "Agent=DQNAgent(state_size, action_size)\n",
    "Train_Agent(Agent, episodes, rewards, state_size)\n",
    "plt.title(\"Cumulative reward per episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cumulative reward\")\n",
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi",
   "language": "python",
   "name": "taxi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
