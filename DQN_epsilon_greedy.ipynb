{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory_size =100_000\n",
    "        self.gamma = 0.95    # 折扣率\n",
    "        self.epsilon = 1.0  # 探索率\n",
    "        self.alpha = 0.001\n",
    "        self.NUM_STEPS_FOR_UPDATE = 4\n",
    "        self.optimizer = Adam(learning_rate = self.alpha)\n",
    "        self.q_network= self._build_network()\n",
    "        self.target_q_network = self._build_network()\n",
    "        \n",
    "    \n",
    "    def _build_network(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.state_size,)))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        return model\n",
    "\n",
    "    def compute_loss(self, experiences):\n",
    "        states, actions, rewards, next_states, done_vals = experiences\n",
    "        max_qsa = tf.reduce_max(self.target_q_network(next_states), axis=-1)\n",
    "        y_targets = rewards + (self.gamma * max_qsa * (1-done_vals))\n",
    "        q_values = self.q_network(states)\n",
    "        q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]), tf.cast(actions, tf.int32)], axis=1))\n",
    "        loss = MSE(y_targets, q_values)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_target_network(self):\n",
    "        TAU=1e-3\n",
    "        for target_weights, q_network_weights in zip(self.target_q_network.weights, self.q_network.weights):\n",
    "            target_weights.assign(TAU * q_network_weights + (1.0-TAU) * target_weights)\n",
    "\n",
    "    @tf.function\n",
    "    def agent_learn(self, experiences):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(experiences)\n",
    "        gradients = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.q_network.trainable_variables))\n",
    "        self.update_target_network()\n",
    "\n",
    "    def get_action(self, q_values, epsilon=0):\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(q_values.numpy()[0])\n",
    "        else:\n",
    "            return random.choice(np.arange(6))\n",
    "\n",
    "    def check_update_conditions(self, j, memory_buffer):\n",
    "        if(j+1) % self.NUM_STEPS_FOR_UPDATE == 0 and len(memory_buffer) > 64:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_experiences(self, memory_buffer):\n",
    "        experiences = random.sample(memory_buffer, k=64)\n",
    "        states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "        done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                                        dtype=tf.float32)\n",
    "        return (states, actions, rewards, next_states, done_vals)\n",
    "\n",
    "    def update_epsilon(self, epsilon):\n",
    "        E_MIN = 0.01\n",
    "        E_DECAY = 0.05\n",
    "        return max(E_MIN, E_DECAY * epsilon)\n",
    "\n",
    "    def get_one_hot_encoding(self, state, next_state, num_states):\n",
    "        state_vector = np.zeros(num_states)\n",
    "        next_state_vector = np.zeros(num_states)\n",
    "        state_vector[state] = 1\n",
    "        next_state_vector[next_state] = 1\n",
    "        \n",
    "        return state_vector, next_state_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Agent(Agent, episodes, rewards, state_size):\n",
    "    memory_buffer = deque(maxlen=Agent.memory_size)\n",
    "    Agent.target_q_network.set_weights(Agent.q_network.get_weights())\n",
    "    epsilon = 1.0\n",
    "    points_history = []\n",
    "    time_step = 1000\n",
    "    experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "     \n",
    "    for i in range(episodes):\n",
    "        observation = env.reset()\n",
    "        state = observation[0]\n",
    "        state, _ = Agent.get_one_hot_encoding(state, 0, state_size)\n",
    "        total_points = 0\n",
    "          \n",
    "        for j in range(time_step):\n",
    "            state_qn = np.expand_dims(state, axis=0)\n",
    "            q_values = Agent.q_network(state_qn)\n",
    "            action = Agent.get_action(q_values, epsilon)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            _, next_state = Agent.get_one_hot_encoding(0, next_state, state_size)\n",
    "            memory_buffer.append(experience(state, action, reward, next_state, terminated))\n",
    "            update = Agent.check_update_conditions(j, memory_buffer)\n",
    "            if update:\n",
    "                experiences = Agent.get_experiences(memory_buffer)\n",
    "                Agent.agent_learn(experiences)\n",
    "            \n",
    "            state = next_state.copy()\n",
    "            total_points += reward\n",
    "\n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "        points_history.append(total_points)\n",
    "        avg_points = np.mean(points_history[-100:])\n",
    "\n",
    "        epsilon = Agent.update_epsilon(epsilon)\n",
    "\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\", end=\"\")\n",
    "\n",
    "\n",
    "        if(avg_points >= 8):\n",
    "            print(f\"Environment solved in {i+1} episodes!\")\n",
    "            break\n",
    "            \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\")\n",
    "        rewards.append(total_points)\n",
    "        \n",
    "    env.close()\n",
    "    print(f\"\\rTraining completed over {episodes} episodes\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.src.backend' has no attribute 'floatx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\CJ\\course\\大四上\\ML AND DM\\Assessment\\CA 2\\Task 1 final\\CS4049-CA2-1\\DQN_epsilon_greedy.ipynb 单元格 4\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CJ/course/%E5%A4%A7%E5%9B%9B%E4%B8%8A/ML%20AND%20DM/Assessment/CA%202/Task%201%20final/CS4049-CA2-1/DQN_epsilon_greedy.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m state_size \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mn\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CJ/course/%E5%A4%A7%E5%9B%9B%E4%B8%8A/ML%20AND%20DM/Assessment/CA%202/Task%201%20final/CS4049-CA2-1/DQN_epsilon_greedy.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m action_size \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CJ/course/%E5%A4%A7%E5%9B%9B%E4%B8%8A/ML%20AND%20DM/Assessment/CA%202/Task%201%20final/CS4049-CA2-1/DQN_epsilon_greedy.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m Agent\u001b[39m=\u001b[39mDQNAgent(state_size, action_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CJ/course/%E5%A4%A7%E5%9B%9B%E4%B8%8A/ML%20AND%20DM/Assessment/CA%202/Task%201%20final/CS4049-CA2-1/DQN_epsilon_greedy.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m Train_Agent(Agent, episodes, rewards, state_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CJ/course/%E5%A4%A7%E5%9B%9B%E4%B8%8A/ML%20AND%20DM/Assessment/CA%202/Task%201%20final/CS4049-CA2-1/DQN_epsilon_greedy.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mCumulative reward per episode\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\CJ\\course\\大四上\\ML AND DM\\Assessment\\CA 2\\Task 1 final\\CS4049-CA2-1\\DQN_epsilon_greedy.ipynb 单元格 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CJ/course/%E5%A4%A7%E5%9B%9B%E4%B8%8A/ML%20AND%20DM/Assessment/CA%202/Task%201%20final/CS4049-CA2-1/DQN_epsilon_greedy.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CJ/course/%E5%A4%A7%E5%9B%9B%E4%B8%8A/ML%20AND%20DM/Assessment/CA%202/Task%201%20final/CS4049-CA2-1/DQN_epsilon_greedy.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNUM_STEPS_FOR_UPDATE \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CJ/course/%E5%A4%A7%E5%9B%9B%E4%B8%8A/ML%20AND%20DM/Assessment/CA%202/Task%201%20final/CS4049-CA2-1/DQN_epsilon_greedy.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m Adam(learning_rate \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CJ/course/%E5%A4%A7%E5%9B%9B%E4%B8%8A/ML%20AND%20DM/Assessment/CA%202/Task%201%20final/CS4049-CA2-1/DQN_epsilon_greedy.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_network\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_network()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CJ/course/%E5%A4%A7%E5%9B%9B%E4%B8%8A/ML%20AND%20DM/Assessment/CA%202/Task%201%20final/CS4049-CA2-1/DQN_epsilon_greedy.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_q_network \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_network()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\adam.py:122\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, name, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     93\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     94\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    109\u001b[0m ):\n\u001b[0;32m    110\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    111\u001b[0m         name\u001b[39m=\u001b[39mname,\n\u001b[0;32m    112\u001b[0m         weight_decay\u001b[39m=\u001b[39mweight_decay,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    121\u001b[0m     )\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_learning_rate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_learning_rate(learning_rate)\n\u001b[0;32m    123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_1 \u001b[39m=\u001b[39m beta_1\n\u001b[0;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_2 \u001b[39m=\u001b[39m beta_2\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1282\u001b[0m, in \u001b[0;36mOptimizer._build_learning_rate\u001b[1;34m(self, learning_rate)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build_learning_rate\u001b[39m(\u001b[39mself\u001b[39m, learning_rate):\n\u001b[0;32m   1281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mesh:\n\u001b[1;32m-> 1282\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_build_learning_rate(learning_rate)\n\u001b[0;32m   1284\u001b[0m     \u001b[39m# For DTensor\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m     variable_creation \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mdtensor\u001b[39m.\u001b[39mDVariable\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:398\u001b[0m, in \u001b[0;36m_BaseOptimizer._build_learning_rate\u001b[1;34m(self, learning_rate)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_learning_rate \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(\n\u001b[0;32m    388\u001b[0m         current_learning_rate,\n\u001b[0;32m    389\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcurrent_learning_rate\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    390\u001b[0m         dtype\u001b[39m=\u001b[39mcurrent_learning_rate\u001b[39m.\u001b[39mdtype,\n\u001b[0;32m    391\u001b[0m         trainable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    392\u001b[0m     )\n\u001b[0;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m learning_rate\n\u001b[0;32m    395\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mVariable(\n\u001b[0;32m    396\u001b[0m     learning_rate,\n\u001b[0;32m    397\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m--> 398\u001b[0m     dtype\u001b[39m=\u001b[39mbackend\u001b[39m.\u001b[39;49mfloatx(),\n\u001b[0;32m    399\u001b[0m     trainable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    400\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.src.backend' has no attribute 'floatx'"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "rewards = []\n",
    "episodes=2500\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "Agent=DQNAgent(state_size, action_size)\n",
    "Train_Agent(Agent, episodes, rewards, state_size)\n",
    "plt.title(\"Cumulative reward per episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cumulative reward\")\n",
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
